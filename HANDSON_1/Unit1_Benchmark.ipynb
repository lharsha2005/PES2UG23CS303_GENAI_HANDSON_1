{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776eb897",
   "metadata": {},
   "source": [
    "Importing requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21ce7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc8ebc",
   "metadata": {},
   "source": [
    "define the path to our course text file (unit 1.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca141fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"unit 1.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da74718",
   "metadata": {},
   "source": [
    "Now we read the file. This text will be the 'Knowledge Base' for our tasks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4449f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fdfc6",
   "metadata": {},
   "source": [
    "**Experiment 1: Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda89bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6346b0",
   "metadata": {},
   "source": [
    "Text Generation using:**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc02653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 664.36it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "The future of Artificial Intelligence is. the some some some some some some some some some some some some some some some some some some some some some some some some some some some some some some some some some some cars them - ( \" but and and and. it it with my my the and in and and and and and - ) ( \" \" ; \" \". and and and and - -, and and and the and the some some some some some some some some more is as that austin for \" (. it was were to whitley whitley whitley whitley many the the some some some some some many the the the the the thele [ the and with him bar as the side or plain t all sound that me for \".. it had with first and and are were hands are and the the the those being so., \". of he do who how a e women you or, - - [, and your this works that i the as within as in all for and and and and and or some some stories, - this or and and in and and - ( with many so so so i as general it everything i as and - - - -, together our. a ( ) ('\". the and and and and and and and in so so \". it that that.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "BERT=pipeline('text-generation', model='bert-base-uncased')\n",
    "output = BERT(prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"-\"*50)\n",
    "print(output[0]['generated_text'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497d072",
   "metadata": {},
   "source": [
    "Text Generation using:**RoBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf0f0eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 632.58it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "The future of Artificial Intelligence is\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "RoBERTa =pipeline('text-generation', model='roberta-base')\n",
    "output = RoBERTa (prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"-\"*50)\n",
    "print(output[0]['generated_text'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70e85",
   "metadata": {},
   "source": [
    "Text Generation using:**BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4755587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 159/159 [00:00<00:00, 791.24it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]   \n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "The future of Artificial Intelligence is55WW Territories Russo Russoigree hiking Reincarn priorit Reincarnhigher derivatives Powell5555 Fist Wood Reincarn Wood Fistimbabwe Woodians WoodIRED Wood Wood outbreak Woodhigher iT GoProsembly ReincarnObs ReincarnHouse monitored Hatt Wood Wood priorit589589589 Wood Wood neglig Wood\\\" ragedTrans55\\\"ians Kardashian priorit\\\" Hatt Wood twisting never WoodObs Wood Segsein Wood Wood Wood leisure twisting589 Wood 700 700 Bei Wood Wood INTOObs Wood iT drawing promotTrans 700 roofs iT Woodians Seg589 Wood\\\" Fist engaging engagingulzulz IAulzruntime Schweulz Wood iT Wood iTulzulztObs engaging Fist Hatt Fist iT pc Fistcipled engaging expresses INTOians\\\"\\\" engagingnitnexpectedlitlit Seg SHEsein\\\" engaging engaging expresses fasc engagingiansiansseinseinTyp monitoredlitlitulz Fisthillary engaging monitoredsein\\\"\\\"1lit translucent iTiansnitians expresses engagingians DeVosians terrorians expresses third engagingiansWashington DeVosMW translucentlitlit translucent expresses dupl expresseslit PSU590 expressesnexpected terrorianslit DeVosians imperfect expressesnexpected expresses expressesnexpectedlitnexpectednexpected engagingObslit expressesians expresseslitiansLC expressesulzlit590iansurablelitulzians CardsLC expresses instructorlitlitlit Corm expressesianslit expresses ancestorsLClitlitWashington engaging expresseslit 700lit expresses expresses drawing engaging expressesnexpectednexpected expressesians Cards expresses\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "BART  =pipeline('text-generation', model='facebook/bart-base')\n",
    "output = BART(prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"-\"*50)\n",
    "print(output[0]['generated_text'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79dd9c",
   "metadata": {},
   "source": [
    "**Experiment 2: Masked Language Modeling (Missing Word)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6fb2f",
   "metadata": {},
   "source": [
    "Missing Word using:**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a9178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 255.67it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0.54\n",
      "generate: 0.16\n",
      "produce: 0.05\n",
      "develop: 0.04\n",
      "add: 0.02\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afafb59",
   "metadata": {},
   "source": [
    "Missing Word using:**RoBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73fce084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 301.57it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate: 0.37\n",
      " create: 0.37\n",
      " discover: 0.08\n",
      " find: 0.02\n",
      " provide: 0.02\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "\n",
    "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09d145",
   "metadata": {},
   "source": [
    "Missing Word using:**BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63db48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:01<00:00, 235.61it/s, Materializing param=model.shared.weight]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create: 0.07\n",
      " help: 0.07\n",
      " provide: 0.06\n",
      " enable: 0.04\n",
      " improve: 0.03\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
    "\n",
    "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e92514",
   "metadata": {},
   "source": [
    "**Experiment 3: Question Answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b03d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the risks?\"\n",
    "]\n",
    "\n",
    "context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c01c18",
   "metadata": {},
   "source": [
    "Question Answereing using: **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42df6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 241.76it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]              \n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: hallucinations\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b74d5",
   "metadata": {},
   "source": [
    "Question Answereing using: **RoBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "609cab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 265.43it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: deepfakes\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463a4a1",
   "metadata": {},
   "source": [
    "Question Answereing using: **BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2cd9237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 311.58it/s, Materializing param=model.shared.weight]                                  \n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.weight | MISSING | \n",
      "qa_outputs.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: deepfakes\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
